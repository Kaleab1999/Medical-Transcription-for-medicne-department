{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kaleab1999/Medical-Transcription-for-medicne-department/blob/main/Africa2_contraceptie_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TUeVhfaidQ-X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12LkdfkVzC-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6c9a5c-edcd-47ec-8b16-e5c41e12a594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S_UR5sFsdYjd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "c2120dd6-0666-47b4-c265-abb659c1a887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-ad44e467662c>:1: DtypeWarning: Columns (1,8,11,13,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df= pd.read_csv('/content/drive/MyDrive/African contr.csv')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Imputation_ Can read part of the sentence  Age  Listening to the radio  \\\n",
              "0            0                             1    1                       1   \n",
              "1            0                                  2                       1   \n",
              "2            0                             1    6                       1   \n",
              "3            0                             1    4                       1   \n",
              "4            0                             1    6                       0   \n",
              "\n",
              "   Ever used internet  Wanted last child then  \\\n",
              "0                   2                       2   \n",
              "1                   2                       2   \n",
              "2                   2                       1   \n",
              "3                   2                       1   \n",
              "4                   2                       1   \n",
              "\n",
              "   Wanted child later or did not want more children  \\\n",
              "0                                                 1   \n",
              "1                                                 1   \n",
              "2                                                 2   \n",
              "3                                                 2   \n",
              "4                                                 2   \n",
              "\n",
              "   Currently using a method to avoid pregnancy  \\\n",
              "0                                            2   \n",
              "1                                            1   \n",
              "2                                            2   \n",
              "3                                            2   \n",
              "4                                            2   \n",
              "\n",
              "  Ever used a method to avoid pregnancy  Wealth index quintile  \\\n",
              "0                                     0                      2   \n",
              "1                                                            1   \n",
              "2                                     0                      2   \n",
              "3                                     0                      3   \n",
              "4                                     1                      3   \n",
              "\n",
              "   Rural wealth index quintile Received prenatal care Place of delivery  \\\n",
              "0                            3                      1                23   \n",
              "1                            2                      1                23   \n",
              "2                            4                      1                22   \n",
              "3                            5                      1                22   \n",
              "4                            4                      1                23   \n",
              "\n",
              "  Mother's health checked after the delivery was over  Education  \\\n",
              "0                                                              1   \n",
              "1                                                              2   \n",
              "2                                                              0   \n",
              "3                                                              0   \n",
              "4                                                              1   \n",
              "\n",
              "   Functional difficulties (age 18-49 years)  Frequency of watching TV  \\\n",
              "0                                          2                         0   \n",
              "1                                          2                         0   \n",
              "2                                          2                         0   \n",
              "3                                          2                         0   \n",
              "4                                          2                         0   \n",
              "\n",
              "   Children ever born Birth Skill attendent  \n",
              "0                   1                     1  \n",
              "1                   1                     1  \n",
              "2                   1                     1  \n",
              "3                   1                     0  \n",
              "4                   2                     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be68758a-ae01-41f1-aed1-82a5ff09e68f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Imputation_</th>\n",
              "      <th>Can read part of the sentence</th>\n",
              "      <th>Age</th>\n",
              "      <th>Listening to the radio</th>\n",
              "      <th>Ever used internet</th>\n",
              "      <th>Wanted last child then</th>\n",
              "      <th>Wanted child later or did not want more children</th>\n",
              "      <th>Currently using a method to avoid pregnancy</th>\n",
              "      <th>Ever used a method to avoid pregnancy</th>\n",
              "      <th>Wealth index quintile</th>\n",
              "      <th>Rural wealth index quintile</th>\n",
              "      <th>Received prenatal care</th>\n",
              "      <th>Place of delivery</th>\n",
              "      <th>Mother's health checked after the delivery was over</th>\n",
              "      <th>Education</th>\n",
              "      <th>Functional difficulties (age 18-49 years)</th>\n",
              "      <th>Frequency of watching TV</th>\n",
              "      <th>Children ever born</th>\n",
              "      <th>Birth Skill attendent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td></td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be68758a-ae01-41f1-aed1-82a5ff09e68f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be68758a-ae01-41f1-aed1-82a5ff09e68f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be68758a-ae01-41f1-aed1-82a5ff09e68f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df= pd.read_csv('/content/drive/MyDrive/African contr.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_7ge2L8WEyNd"
      },
      "outputs": [],
      "source": [
        "df4= df[df['Imputation_']==5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_VZLC21-HwjY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ae2cce8b-afb0-41ea-e104-93d7aaf548d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Imputation_ Can read part of the sentence  Age  \\\n",
              "262610            5                             1    1   \n",
              "262611            5                             0    2   \n",
              "262612            5                             1    6   \n",
              "262613            5                             1    4   \n",
              "262614            5                             1    6   \n",
              "\n",
              "        Listening to the radio  Ever used internet  Wanted last child then  \\\n",
              "262610                       1                   0                       0   \n",
              "262611                       1                   0                       0   \n",
              "262612                       1                   0                       1   \n",
              "262613                       1                   0                       1   \n",
              "262614                       0                   0                       1   \n",
              "\n",
              "        Wanted child later or did not want more children  \\\n",
              "262610                                                 1   \n",
              "262611                                                 1   \n",
              "262612                                                 0   \n",
              "262613                                                 0   \n",
              "262614                                                 0   \n",
              "\n",
              "        Currently using a method to avoid pregnancy  \\\n",
              "262610                                            0   \n",
              "262611                                            1   \n",
              "262612                                            0   \n",
              "262613                                            0   \n",
              "262614                                            0   \n",
              "\n",
              "       Ever used a method to avoid pregnancy  Wealth index quintile  \\\n",
              "262610                                     0                      2   \n",
              "262611                                     1                      1   \n",
              "262612                                     0                      2   \n",
              "262613                                     0                      3   \n",
              "262614                                     1                      3   \n",
              "\n",
              "        Rural wealth index quintile Received prenatal care Place of delivery  \\\n",
              "262610                            3                      1                23   \n",
              "262611                            2                      1                23   \n",
              "262612                            4                      1                22   \n",
              "262613                            5                      1                22   \n",
              "262614                            4                      1                23   \n",
              "\n",
              "       Mother's health checked after the delivery was over  Education  \\\n",
              "262610                                                  0           1   \n",
              "262611                                                  0           2   \n",
              "262612                                                  0           0   \n",
              "262613                                                  0           0   \n",
              "262614                                                  1           1   \n",
              "\n",
              "        Functional difficulties (age 18-49 years)  Frequency of watching TV  \\\n",
              "262610                                          0                         0   \n",
              "262611                                          0                         0   \n",
              "262612                                          0                         0   \n",
              "262613                                          0                         0   \n",
              "262614                                          0                         0   \n",
              "\n",
              "        Children ever born Birth Skill attendent  \n",
              "262610                   1                     1  \n",
              "262611                   1                     1  \n",
              "262612                   1                     1  \n",
              "262613                   1                     0  \n",
              "262614                   2                     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8cb9b0d-8263-4f67-906a-c7e63c471fda\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Imputation_</th>\n",
              "      <th>Can read part of the sentence</th>\n",
              "      <th>Age</th>\n",
              "      <th>Listening to the radio</th>\n",
              "      <th>Ever used internet</th>\n",
              "      <th>Wanted last child then</th>\n",
              "      <th>Wanted child later or did not want more children</th>\n",
              "      <th>Currently using a method to avoid pregnancy</th>\n",
              "      <th>Ever used a method to avoid pregnancy</th>\n",
              "      <th>Wealth index quintile</th>\n",
              "      <th>Rural wealth index quintile</th>\n",
              "      <th>Received prenatal care</th>\n",
              "      <th>Place of delivery</th>\n",
              "      <th>Mother's health checked after the delivery was over</th>\n",
              "      <th>Education</th>\n",
              "      <th>Functional difficulties (age 18-49 years)</th>\n",
              "      <th>Frequency of watching TV</th>\n",
              "      <th>Children ever born</th>\n",
              "      <th>Birth Skill attendent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>262610</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262611</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262612</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262613</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262614</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8cb9b0d-8263-4f67-906a-c7e63c471fda')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8cb9b0d-8263-4f67-906a-c7e63c471fda button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8cb9b0d-8263-4f67-906a-c7e63c471fda');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q4jc-QWLdyyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "115bbf68-7b4a-40ed-cb51-eeb777dab455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 52522 entries, 262610 to 315131\n",
            "Data columns (total 19 columns):\n",
            " #   Column                                               Non-Null Count  Dtype \n",
            "---  ------                                               --------------  ----- \n",
            " 0   Imputation_                                          52522 non-null  int64 \n",
            " 1   Can read part of the sentence                        52522 non-null  object\n",
            " 2   Age                                                  52522 non-null  int64 \n",
            " 3   Listening to the radio                               52522 non-null  int64 \n",
            " 4   Ever used internet                                   52522 non-null  int64 \n",
            " 5   Wanted last child then                               52522 non-null  int64 \n",
            " 6   Wanted child later or did not want more children     52522 non-null  int64 \n",
            " 7   Currently using a method to avoid pregnancy          52522 non-null  int64 \n",
            " 8   Ever used a method to avoid pregnancy                52522 non-null  object\n",
            " 9   Wealth index quintile                                52522 non-null  int64 \n",
            " 10  Rural wealth index quintile                          52522 non-null  int64 \n",
            " 11  Received prenatal care                               52522 non-null  object\n",
            " 12  Place of delivery                                    52522 non-null  object\n",
            " 13  Mother's health checked after the delivery was over  52522 non-null  object\n",
            " 14  Education                                            52522 non-null  int64 \n",
            " 15  Functional difficulties (age 18-49 years)            52522 non-null  int64 \n",
            " 16  Frequency of watching TV                             52522 non-null  int64 \n",
            " 17  Children ever born                                   52522 non-null  int64 \n",
            " 18  Birth Skill attendent                                52522 non-null  object\n",
            "dtypes: int64(13), object(6)\n",
            "memory usage: 8.0+ MB\n"
          ]
        }
      ],
      "source": [
        "df4.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQOHGm4Pusc6"
      },
      "source": [
        "#### missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Rd8IhSTLd2Ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ad959f-db5a-43a2-cfb1-510c8d31edd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 number of columns has missing values\n"
          ]
        }
      ],
      "source": [
        "su= 0\n",
        "for i in df4.columns:\n",
        "  if df4[i].isnull().sum()>0:\n",
        "    su+=1\n",
        "    print(\"{} has {} missing values\".format(i,df4[i].isnull().sum()))\n",
        "\n",
        "print(\"{} number of columns has missing values\".format(su))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "32GA7JjAeVBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419aa90b-8a97-4e69-be6e-9c5b04c7878d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Imputation_                                            0\n",
              "Can read part of the sentence                          0\n",
              "Age                                                    0\n",
              "Listening to the radio                                 0\n",
              "Ever used internet                                     0\n",
              "Wanted last child then                                 0\n",
              "Wanted child later or did not want more children       0\n",
              "Currently using a method to avoid pregnancy            0\n",
              "Ever used a method to avoid pregnancy                  0\n",
              "Wealth index quintile                                  0\n",
              "Rural wealth index quintile                            0\n",
              "Received prenatal care                                 0\n",
              "Place of delivery                                      0\n",
              "Mother's health checked after the delivery was over    0\n",
              "Education                                              0\n",
              "Functional difficulties (age 18-49 years)              0\n",
              "Frequency of watching TV                               0\n",
              "Children ever born                                     0\n",
              "Birth Skill attendent                                  0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df4.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "It8mUoXxexaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c111dfa-8fe4-4b1a-9255-e412bccb511b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Imputation_                                             1\n",
              "Can read part of the sentence                           2\n",
              "Age                                                     7\n",
              "Listening to the radio                                  2\n",
              "Ever used internet                                      2\n",
              "Wanted last child then                                  2\n",
              "Wanted child later or did not want more children        2\n",
              "Currently using a method to avoid pregnancy             2\n",
              "Ever used a method to avoid pregnancy                   2\n",
              "Wealth index quintile                                   5\n",
              "Rural wealth index quintile                             5\n",
              "Received prenatal care                                  2\n",
              "Place of delivery                                      20\n",
              "Mother's health checked after the delivery was over     2\n",
              "Education                                               4\n",
              "Functional difficulties (age 18-49 years)               2\n",
              "Frequency of watching TV                                2\n",
              "Children ever born                                      3\n",
              "Birth Skill attendent                                   2\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df4.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ff_mnbj7e1du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18bf3650-e454-417a-b3c6-5da68f7eed7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    38957\n",
              "0    13565\n",
              "Name: Can read part of the sentence, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df4['Can read part of the sentence'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cjnk2416fVai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02366b91-daaa-497d-cc60-8d264ede62d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    46538\n",
              "1     5984\n",
              "Name: Ever used a method to avoid pregnancy, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df4['Ever used a method to avoid pregnancy'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uVRUT7YWgC51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664edf46-f189-442d-c663-06ef30b39e71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    44000\n",
              "0     8522\n",
              "Name: Received prenatal care, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "df4['Received prenatal care'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lWDcfDfOhTdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f42885a-6329-405c-a94e-2e766c92dc86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    41517\n",
              "1    11005\n",
              "Name: Mother's health checked after the delivery was over, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df4[\"Mother's health checked after the delivery was over\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "87flKMCrhhZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98259ad-159f-49e0-a971-c0eb66d7122e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    32516\n",
              "0    20006\n",
              "Name: Birth Skill attendent, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df4[\"Birth Skill attendent\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "R374XMEyJ7ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a3b632-fd13-47c0-853e-fbad37295734"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11    22158\n",
              "22     9740\n",
              "21     7579\n",
              "23     6679\n",
              "12     3228\n",
              "31      819\n",
              "41      452\n",
              "32      444\n",
              "96      413\n",
              "33      270\n",
              "26      260\n",
              "36      242\n",
              "42      154\n",
              "99       26\n",
              "24       22\n",
              "35       12\n",
              "34       10\n",
              "43        6\n",
              "76        5\n",
              "          3\n",
              "Name: Place of delivery, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df4['Place of delivery'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h4nj-CVlIzNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ca8158-7545-421d-b6d0-6f151472b4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-ed6431e8a917>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Place of delivery'] = df4['Place of delivery'].replace(replacements, inplace=False)\n"
          ]
        }
      ],
      "source": [
        "# define the values to be replaced and their replacement values\n",
        "replacements = {'96': 0, '11': 0, '12': 0, '99': 0, \n",
        "                '21': 1, '22': 1, '23': 1, '24': 1,\n",
        "                '26': 1, '31': 1, '32': 1, '33': 1,\n",
        "                '34': 1, '35': 1, '36': 1, '41': 1,\n",
        "                '42': 1, '43': 1, '76': 1, ' ': 1}\n",
        "\n",
        "# replace values using the replace method and specify 'inplace=False'\n",
        "df4['Place of delivery'] = df4['Place of delivery'].replace(replacements, inplace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "riDWJJYvPKiX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e9648287-842c-4ec9-b425-8f000ac36991"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Imputation_ Can read part of the sentence  Age  \\\n",
              "262610            5                             1    1   \n",
              "262611            5                             0    2   \n",
              "262612            5                             1    6   \n",
              "262613            5                             1    4   \n",
              "262614            5                             1    6   \n",
              "\n",
              "        Listening to the radio  Ever used internet  Wanted last child then  \\\n",
              "262610                       1                   0                       0   \n",
              "262611                       1                   0                       0   \n",
              "262612                       1                   0                       1   \n",
              "262613                       1                   0                       1   \n",
              "262614                       0                   0                       1   \n",
              "\n",
              "        Wanted child later or did not want more children  \\\n",
              "262610                                                 1   \n",
              "262611                                                 1   \n",
              "262612                                                 0   \n",
              "262613                                                 0   \n",
              "262614                                                 0   \n",
              "\n",
              "        Currently using a method to avoid pregnancy  \\\n",
              "262610                                            0   \n",
              "262611                                            1   \n",
              "262612                                            0   \n",
              "262613                                            0   \n",
              "262614                                            0   \n",
              "\n",
              "       Ever used a method to avoid pregnancy  Wealth index quintile  \\\n",
              "262610                                     0                      2   \n",
              "262611                                     1                      1   \n",
              "262612                                     0                      2   \n",
              "262613                                     0                      3   \n",
              "262614                                     1                      3   \n",
              "\n",
              "        Rural wealth index quintile Received prenatal care  Place of delivery  \\\n",
              "262610                            3                      1                  1   \n",
              "262611                            2                      1                  1   \n",
              "262612                            4                      1                  1   \n",
              "262613                            5                      1                  1   \n",
              "262614                            4                      1                  1   \n",
              "\n",
              "       Mother's health checked after the delivery was over  Education  \\\n",
              "262610                                                  0           1   \n",
              "262611                                                  0           2   \n",
              "262612                                                  0           0   \n",
              "262613                                                  0           0   \n",
              "262614                                                  1           1   \n",
              "\n",
              "        Functional difficulties (age 18-49 years)  Frequency of watching TV  \\\n",
              "262610                                          0                         0   \n",
              "262611                                          0                         0   \n",
              "262612                                          0                         0   \n",
              "262613                                          0                         0   \n",
              "262614                                          0                         0   \n",
              "\n",
              "        Children ever born Birth Skill attendent  \n",
              "262610                   1                     1  \n",
              "262611                   1                     1  \n",
              "262612                   1                     1  \n",
              "262613                   1                     0  \n",
              "262614                   2                     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4215c7e3-b67d-49c4-be4e-0a2096b0d7b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Imputation_</th>\n",
              "      <th>Can read part of the sentence</th>\n",
              "      <th>Age</th>\n",
              "      <th>Listening to the radio</th>\n",
              "      <th>Ever used internet</th>\n",
              "      <th>Wanted last child then</th>\n",
              "      <th>Wanted child later or did not want more children</th>\n",
              "      <th>Currently using a method to avoid pregnancy</th>\n",
              "      <th>Ever used a method to avoid pregnancy</th>\n",
              "      <th>Wealth index quintile</th>\n",
              "      <th>Rural wealth index quintile</th>\n",
              "      <th>Received prenatal care</th>\n",
              "      <th>Place of delivery</th>\n",
              "      <th>Mother's health checked after the delivery was over</th>\n",
              "      <th>Education</th>\n",
              "      <th>Functional difficulties (age 18-49 years)</th>\n",
              "      <th>Frequency of watching TV</th>\n",
              "      <th>Children ever born</th>\n",
              "      <th>Birth Skill attendent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>262610</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262611</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262612</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262613</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262614</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4215c7e3-b67d-49c4-be4e-0a2096b0d7b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4215c7e3-b67d-49c4-be4e-0a2096b0d7b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4215c7e3-b67d-49c4-be4e-0a2096b0d7b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja7bckzaCbvS"
      },
      "source": [
        "# remove common values (i.e., imputations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aW1mHkLRSorR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3098a4ed-bc90-468e-9a8e-df07f99fd02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-259e89279ca7>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4.drop(['Imputation_'], axis=1, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "df4.drop(['Imputation_'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2AA0f-V6CqWE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0b521363-2680-44a3-ee34-409bcdf6d5e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Can read part of the sentence  Age  Listening to the radio  \\\n",
              "262610                             1    1                       1   \n",
              "262611                             0    2                       1   \n",
              "262612                             1    6                       1   \n",
              "262613                             1    4                       1   \n",
              "262614                             1    6                       0   \n",
              "\n",
              "        Ever used internet  Wanted last child then  \\\n",
              "262610                   0                       0   \n",
              "262611                   0                       0   \n",
              "262612                   0                       1   \n",
              "262613                   0                       1   \n",
              "262614                   0                       1   \n",
              "\n",
              "        Wanted child later or did not want more children  \\\n",
              "262610                                                 1   \n",
              "262611                                                 1   \n",
              "262612                                                 0   \n",
              "262613                                                 0   \n",
              "262614                                                 0   \n",
              "\n",
              "        Currently using a method to avoid pregnancy  \\\n",
              "262610                                            0   \n",
              "262611                                            1   \n",
              "262612                                            0   \n",
              "262613                                            0   \n",
              "262614                                            0   \n",
              "\n",
              "       Ever used a method to avoid pregnancy  Wealth index quintile  \\\n",
              "262610                                     0                      2   \n",
              "262611                                     1                      1   \n",
              "262612                                     0                      2   \n",
              "262613                                     0                      3   \n",
              "262614                                     1                      3   \n",
              "\n",
              "        Rural wealth index quintile Received prenatal care  Place of delivery  \\\n",
              "262610                            3                      1                  1   \n",
              "262611                            2                      1                  1   \n",
              "262612                            4                      1                  1   \n",
              "262613                            5                      1                  1   \n",
              "262614                            4                      1                  1   \n",
              "\n",
              "       Mother's health checked after the delivery was over  Education  \\\n",
              "262610                                                  0           1   \n",
              "262611                                                  0           2   \n",
              "262612                                                  0           0   \n",
              "262613                                                  0           0   \n",
              "262614                                                  1           1   \n",
              "\n",
              "        Functional difficulties (age 18-49 years)  Frequency of watching TV  \\\n",
              "262610                                          0                         0   \n",
              "262611                                          0                         0   \n",
              "262612                                          0                         0   \n",
              "262613                                          0                         0   \n",
              "262614                                          0                         0   \n",
              "\n",
              "        Children ever born Birth Skill attendent  \n",
              "262610                   1                     1  \n",
              "262611                   1                     1  \n",
              "262612                   1                     1  \n",
              "262613                   1                     0  \n",
              "262614                   2                     1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d9959f5-fd05-4bf6-92b3-3b5c49a99948\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Can read part of the sentence</th>\n",
              "      <th>Age</th>\n",
              "      <th>Listening to the radio</th>\n",
              "      <th>Ever used internet</th>\n",
              "      <th>Wanted last child then</th>\n",
              "      <th>Wanted child later or did not want more children</th>\n",
              "      <th>Currently using a method to avoid pregnancy</th>\n",
              "      <th>Ever used a method to avoid pregnancy</th>\n",
              "      <th>Wealth index quintile</th>\n",
              "      <th>Rural wealth index quintile</th>\n",
              "      <th>Received prenatal care</th>\n",
              "      <th>Place of delivery</th>\n",
              "      <th>Mother's health checked after the delivery was over</th>\n",
              "      <th>Education</th>\n",
              "      <th>Functional difficulties (age 18-49 years)</th>\n",
              "      <th>Frequency of watching TV</th>\n",
              "      <th>Children ever born</th>\n",
              "      <th>Birth Skill attendent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>262610</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262611</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262612</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262613</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262614</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d9959f5-fd05-4bf6-92b3-3b5c49a99948')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d9959f5-fd05-4bf6-92b3-3b5c49a99948 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d9959f5-fd05-4bf6-92b3-3b5c49a99948');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ze59w65OCuZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a819b1-75b2-4033-87e1-95ab3db9cb98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-1e1dc18d3adb>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Can read part of the sentence'] = df4['Can read part of the sentence'].astype(int)\n",
            "<ipython-input-20-1e1dc18d3adb>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Ever used a method to avoid pregnancy'] = df4['Ever used a method to avoid pregnancy'].astype(int)\n",
            "<ipython-input-20-1e1dc18d3adb>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Received prenatal care'] = df4['Received prenatal care'].astype(int)\n",
            "<ipython-input-20-1e1dc18d3adb>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4[\"Mother's health checked after the delivery was over\"] = df4[\"Mother's health checked after the delivery was over\"].astype(int)\n",
            "<ipython-input-20-1e1dc18d3adb>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Birth Skill attendent'] = df4['Birth Skill attendent'].astype(int)\n",
            "<ipython-input-20-1e1dc18d3adb>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4['Place of delivery'] = df4['Place of delivery'].astype(int)\n"
          ]
        }
      ],
      "source": [
        "df4['Can read part of the sentence'] = df4['Can read part of the sentence'].astype(int)\n",
        "df4['Ever used a method to avoid pregnancy'] = df4['Ever used a method to avoid pregnancy'].astype(int)\n",
        "df4['Received prenatal care'] = df4['Received prenatal care'].astype(int)\n",
        "df4[\"Mother's health checked after the delivery was over\"] = df4[\"Mother's health checked after the delivery was over\"].astype(int)\n",
        "df4['Birth Skill attendent'] = df4['Birth Skill attendent'].astype(int)\n",
        "df4['Place of delivery'] = df4['Place of delivery'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sHXnGOL6ESlq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301ef8fb-5c77-4d48-df29-12544fb3fb41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 52522 entries, 262610 to 315131\n",
            "Data columns (total 18 columns):\n",
            " #   Column                                               Non-Null Count  Dtype\n",
            "---  ------                                               --------------  -----\n",
            " 0   Can read part of the sentence                        52522 non-null  int64\n",
            " 1   Age                                                  52522 non-null  int64\n",
            " 2   Listening to the radio                               52522 non-null  int64\n",
            " 3   Ever used internet                                   52522 non-null  int64\n",
            " 4   Wanted last child then                               52522 non-null  int64\n",
            " 5   Wanted child later or did not want more children     52522 non-null  int64\n",
            " 6   Currently using a method to avoid pregnancy          52522 non-null  int64\n",
            " 7   Ever used a method to avoid pregnancy                52522 non-null  int64\n",
            " 8   Wealth index quintile                                52522 non-null  int64\n",
            " 9   Rural wealth index quintile                          52522 non-null  int64\n",
            " 10  Received prenatal care                               52522 non-null  int64\n",
            " 11  Place of delivery                                    52522 non-null  int64\n",
            " 12  Mother's health checked after the delivery was over  52522 non-null  int64\n",
            " 13  Education                                            52522 non-null  int64\n",
            " 14  Functional difficulties (age 18-49 years)            52522 non-null  int64\n",
            " 15  Frequency of watching TV                             52522 non-null  int64\n",
            " 16  Children ever born                                   52522 non-null  int64\n",
            " 17  Birth Skill attendent                                52522 non-null  int64\n",
            "dtypes: int64(18)\n",
            "memory usage: 7.6 MB\n"
          ]
        }
      ],
      "source": [
        "df4.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_5bf9sc1EUUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33533c32-8f04-4b60-d47a-4427a1b11630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-1d7cff8ca81b>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4.drop(['Currently using a method to avoid pregnancy'],axis=1, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "y= list(df4['Currently using a method to avoid pregnancy'])\n",
        "df4.drop(['Currently using a method to avoid pregnancy'],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FsdPbG6KG03G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "7b620580-51ba-4a1d-8391-8efe4b1fe8ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Can read part of the sentence  Age  Listening to the radio  \\\n",
              "262610                              1    1                       1   \n",
              "262611                              0    2                       1   \n",
              "262612                              1    6                       1   \n",
              "262613                              1    4                       1   \n",
              "262614                              1    6                       0   \n",
              "\n",
              "        Ever used internet  Wanted last child then  \\\n",
              "262610                   0                       0   \n",
              "262611                   0                       0   \n",
              "262612                   0                       1   \n",
              "262613                   0                       1   \n",
              "262614                   0                       1   \n",
              "\n",
              "        Wanted child later or did not want more children  \\\n",
              "262610                                                 1   \n",
              "262611                                                 1   \n",
              "262612                                                 0   \n",
              "262613                                                 0   \n",
              "262614                                                 0   \n",
              "\n",
              "        Ever used a method to avoid pregnancy  Wealth index quintile  \\\n",
              "262610                                      0                      2   \n",
              "262611                                      1                      1   \n",
              "262612                                      0                      2   \n",
              "262613                                      0                      3   \n",
              "262614                                      1                      3   \n",
              "\n",
              "        Rural wealth index quintile  Received prenatal care  \\\n",
              "262610                            3                       1   \n",
              "262611                            2                       1   \n",
              "262612                            4                       1   \n",
              "262613                            5                       1   \n",
              "262614                            4                       1   \n",
              "\n",
              "        Place of delivery  \\\n",
              "262610                  1   \n",
              "262611                  1   \n",
              "262612                  1   \n",
              "262613                  1   \n",
              "262614                  1   \n",
              "\n",
              "        Mother's health checked after the delivery was over  Education  \\\n",
              "262610                                                  0            1   \n",
              "262611                                                  0            2   \n",
              "262612                                                  0            0   \n",
              "262613                                                  0            0   \n",
              "262614                                                  1            1   \n",
              "\n",
              "        Functional difficulties (age 18-49 years)  Frequency of watching TV  \\\n",
              "262610                                          0                         0   \n",
              "262611                                          0                         0   \n",
              "262612                                          0                         0   \n",
              "262613                                          0                         0   \n",
              "262614                                          0                         0   \n",
              "\n",
              "        Children ever born  Birth Skill attendent  \n",
              "262610                   1                      1  \n",
              "262611                   1                      1  \n",
              "262612                   1                      1  \n",
              "262613                   1                      0  \n",
              "262614                   2                      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c5ae192-0378-4fdc-a7c0-e1d20c4ca140\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Can read part of the sentence</th>\n",
              "      <th>Age</th>\n",
              "      <th>Listening to the radio</th>\n",
              "      <th>Ever used internet</th>\n",
              "      <th>Wanted last child then</th>\n",
              "      <th>Wanted child later or did not want more children</th>\n",
              "      <th>Ever used a method to avoid pregnancy</th>\n",
              "      <th>Wealth index quintile</th>\n",
              "      <th>Rural wealth index quintile</th>\n",
              "      <th>Received prenatal care</th>\n",
              "      <th>Place of delivery</th>\n",
              "      <th>Mother's health checked after the delivery was over</th>\n",
              "      <th>Education</th>\n",
              "      <th>Functional difficulties (age 18-49 years)</th>\n",
              "      <th>Frequency of watching TV</th>\n",
              "      <th>Children ever born</th>\n",
              "      <th>Birth Skill attendent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>262610</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262611</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262612</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262613</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262614</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c5ae192-0378-4fdc-a7c0-e1d20c4ca140')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c5ae192-0378-4fdc-a7c0-e1d20c4ca140 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c5ae192-0378-4fdc-a7c0-e1d20c4ca140');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df4.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OK3E91qg_ZkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e04d230-c96d-4785-89c7-b235d94ae5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-57807c46932a>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df4.drop(['Birth Skill attendent'],axis=1, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "df4.drop(['Birth Skill attendent'],axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dwCnXUwUEuXu"
      },
      "outputs": [],
      "source": [
        "X= df4.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GHv8EoWZn_Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561fec4e-0cb7-4796-acbe-4200ab83a7fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 39629, 1: 12893})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "from collections import Counter\n",
        "counts = Counter(y)\n",
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ssPai4RKaRpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb8ace2-9948-4407-d884-8cca9724ddb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=1)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Be3rMzAsG5Qe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aoqpDeoVHGia"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "T3XMBnWrpbBu"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JkKMJbRlUC5e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uEZ0P3yCIvyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52cffe9d-420b-456a-d192-bebf188e0434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "rf.fit(X_train1, y_train1)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Zc1W3lnfQY2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "278dd3ea-e99b-40ff-991b-e8f11f0580f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "AUC:\n",
            " 0.6300982573229514\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "AUC: 0.6236963463436027\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZND0ICMsF53"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Pg3X_swsQrxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4386915d-fef2-4f4e-c4dd-9b820f6c81ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "AUC: 0.6300982573229514\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "AUC: 0.6236963463436027\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "\n",
        "xgb_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_clf, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "m5saoyGLK6wG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a0f37d-a8ff-4fa7-c6be-7ead245dde82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.61898833 0.6263035  0.6244358  0.63097276 0.62225681 0.62614786\n",
            " 0.62568093 0.62396887 0.62157534 0.63060399]\n",
            "Average cross-validation score: 0.6250934191335024\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yJ2fHqzl-_YT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ac33f9-eb73-43ad-94aa-b05b787f68a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "001stqtBGMmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb293bd-501b-4c9f-bcd7-96d8a53e21d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KTUSYfe1ATGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de29f22-6a1f-4a94-bfc7-53652ea26917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "MGvw-H8hG6Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5003a9a1-03ea-40af-df2f-72780c5b71bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2nMKqNT3Hicf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1111a4-c15b-4330-eb1e-459bd4096e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62272374 0.62754864 0.62428016 0.62487353 0.62604094]\n",
            "Average cross-validation score: 0.6250933994469736\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "t6y0ci2aIOo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a1d48af-6760-4f65-9b19-39db010c65df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.61898833 0.6263035  0.6244358  0.63097276 0.62225681 0.62614786\n",
            " 0.62568093 0.62396887 0.62157534 0.63060399]\n",
            "Average cross-validation score: 0.6250934191335024\n",
            "================== Training===================\n",
            "Accuracy training: 0.5870530992172626\n",
            "Precision training: 0.3312782572955199\n",
            "Recall training: 0.7126436781609196\n",
            "F1 score training: 0.4523007856341189\n",
            "Confusion matrix:\n",
            " [[1969 1627]\n",
            " [ 325  806]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5830953740719589\n",
            "Precision testing: 0.34595775673707213\n",
            "Recall testing: 0.7068452380952381\n",
            "F1 score testing: 0.4645476772616137\n",
            "Confusion matrix:\n",
            " [[2113 1796]\n",
            " [ 394  950]]\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btQidWX_9CUP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=2)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPx9_O8u9eDQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z9VCPbF9eDQ"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ua0VFYE9eDQ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr741HL-9eDQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-ZQoJPt9eDR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOGPD3Pv9eDR"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wrEbA0M9eDS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yadfZVG49eDS"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZq-MxL99eDS"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfYiVxFbNg9u"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wv1xdOSNg9u"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yViaodjuNg9u"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiEhmzfjNg9v"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyP96Y-rNg9v"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNseVFjjNg9v"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EirgPQOV94FD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebbb52c-1930-4e17-94c4-e86c19003770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0 10 12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=3)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "GF8Peau19hok"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lIkN0V9x9hol"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6HDgS5_H9hol"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "aUpdAm6K9hol"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7dGXnNJO9hol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29530811-37bf-4460-a3ac-0c18772686e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
            "Best hyperparameters: {'n_estimators': 180, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 20, 'criterion': 'gini'}\n",
            "Cross-validation scores: [0.64460174 0.64869701 0.65009724 0.64262933 0.64558538]\n",
            "Average cross-validation score: 0.6463221376625068\n",
            "================== Training===================\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AIpuhcmh9hol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07918579-5b57-4588-cdf6-58f6a99107bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.63814561 0.65105787 0.64187928 0.65556938 0.64312383 0.65712508\n",
            " 0.63793372 0.64540221 0.64493543 0.64618018]\n",
            "Average cross-validation score: 0.6461352590468669\n",
            "================== Training===================\n",
            "\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "AUC:\n",
            " 0.6441848544752695\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "AUC: 0.6539069935450228\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "DVzq5amF9hol"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TPkOJjvX9hol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20c9dfc-b611-4bca-ac83-cfeb93fcbe8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.64460174 0.64869701 0.65009724 0.64262933 0.64558538]\n",
            "Average cross-validation score: 0.6463221376625068\n",
            "================== Training===================\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "AUC: 0.6441848544752695\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "AUC: 0.6539069935450228\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9hjHBtu39hom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702e974c-8d28-48dc-ed9e-dd233e38668e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62616677 0.62476665 0.6295893  0.6295893  0.63130056 0.64452396\n",
            " 0.62673098 0.61521705 0.63139879 0.62673098]\n",
            "Average cross-validation score: 0.6286014324486122\n",
            "================== Training===================\n",
            "Accuracy training: 0.5885339538819547\n",
            "Precision training: 0.3407190635451505\n",
            "Recall training: 0.6889264581572274\n",
            "F1 score training: 0.45594405594405596\n",
            "Confusion matrix:\n",
            " [[1967 1577]\n",
            " [ 368  815]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6053683609366076\n",
            "Precision testing: 0.35467980295566504\n",
            "Recall testing: 0.7166921898928025\n",
            "F1 score testing: 0.47452471482889735\n",
            "Confusion matrix:\n",
            " [[2244 1703]\n",
            " [ 370  936]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GId-PeQ8Nroj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed584471-78f8-4731-f300-6f9f8bb1c081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.64413503 0.64939712 0.65025282 0.63990665 0.64294049]\n",
            "Average cross-validation score: 0.6453264234144811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================== Training===================\n",
            "Accuracy training: 0.668077004442564\n",
            "Precision training: 0.3934878587196468\n",
            "Recall training: 0.6027049873203719\n",
            "F1 score training: 0.4761268781302171\n",
            "Confusion matrix:\n",
            " [[2445 1099]\n",
            " [ 470  713]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.67390062821245\n",
            "Precision testing: 0.3990074441687345\n",
            "Recall testing: 0.6156202143950995\n",
            "F1 score testing: 0.48419150858175247\n",
            "Confusion matrix:\n",
            " [[2736 1211]\n",
            " [ 502  804]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tRrvNACzNrok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4392146-5add-4a92-c496-9ee1b104c4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.64009023 0.64542979 0.64830805 0.63858421 0.64146247]\n",
            "Average cross-validation score: 0.6427749499769702\n",
            "================== Training===================\n",
            "Accuracy training: 0.6519991537973344\n",
            "Precision training: 0.38117283950617287\n",
            "Recall training: 0.6263736263736264\n",
            "F1 score training: 0.47393668052446436\n",
            "Confusion matrix:\n",
            " [[2341 1203]\n",
            " [ 442  741]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6607652769845802\n",
            "Precision testing: 0.38940520446096655\n",
            "Recall testing: 0.6416539050535988\n",
            "F1 score testing: 0.48467322151532677\n",
            "Confusion matrix:\n",
            " [[2633 1314]\n",
            " [ 468  838]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "F473tfhnNrok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ccb33f-ea2e-4c48-939f-d459cc767a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.62243311 0.62660443 0.63562816 0.61921431 0.62707118]\n",
            "Average cross-validation score: 0.6261902383025256\n",
            "================== Training===================\n",
            "Accuracy training: 0.5737254072350327\n",
            "Precision training: 0.3375\n",
            "Recall training: 0.7303465765004227\n",
            "F1 score training: 0.4616617686347849\n",
            "Confusion matrix:\n",
            " [[1848 1696]\n",
            " [ 319  864]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.5897582333904435\n",
            "Precision testing: 0.3489861259338314\n",
            "Recall testing: 0.7511485451761103\n",
            "F1 score testing: 0.47656060238037407\n",
            "Confusion matrix:\n",
            " [[2117 1830]\n",
            " [ 325  981]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mXUXvG9RNrol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188d567d-2d3c-4171-ceda-aab02b9fd32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.64460174 0.64869701 0.65009724 0.64262933 0.64558538]\n",
            "Average cross-validation score: 0.6463221376625068\n",
            "================== Training===================\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "94U5m5AYNrol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5ee46b-2425-43a0-99b6-0132b40ac455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.64460174 0.63508363 0.65009724 0.64262933 0.64558538]\n",
            "Average cross-validation score: 0.6435994616609509\n",
            "================== Training===================\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "OZDEpltRNrom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d438e485-58de-4d7c-c14c-c6f46ce47eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.63814561 0.65105787 0.64187928 0.65556938 0.64312383 0.65712508\n",
            " 0.63793372 0.64540221 0.64493543 0.64618018]\n",
            "Average cross-validation score: 0.6461352590468669\n",
            "================== Training===================\n",
            "Accuracy training: 0.6644806431140258\n",
            "Precision training: 0.3899508465319498\n",
            "Recall training: 0.6035502958579881\n",
            "F1 score training: 0.4737889847378899\n",
            "Confusion matrix:\n",
            " [[2427 1117]\n",
            " [ 469  714]]\n",
            "================== Testing===================\n",
            "Accuracy testing: 0.6716162193032553\n",
            "Precision testing: 0.39705159705159704\n",
            "Recall testing: 0.6186830015313936\n",
            "F1 score testing: 0.4836875187069739\n",
            "Confusion matrix:\n",
            " [[2720 1227]\n",
            " [ 498  808]]\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "8bMV6Y1IPKv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec79028-c71b-4b2d-907c-6d131e4d5834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  2  3  5  6  7  8  9 10 11 12 14]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=12)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "diDmX4t09lKN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ctFX-oM-9lKO"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8HBV1xI_9lKO"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ARqhyQD39lKO"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tim_cN_g9lKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec419f0-e8ec-4b63-d2d6-062a41bbdc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NvlAWDO9lKO"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tidXAfuU9lKP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJM35GBR9lKP"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITzqvqEj9lKP"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyqTgwdMNz4I"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGEDD-WxNz4J"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTTCK550Nz4J"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZxJ_4ksNz4J"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_HM1nuLNz4K"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYKNNqRINz4K"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy8eP1w997yC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=4)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXquHBJF9mFB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NM3vQFe9mFB"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqXlac_L9mFC"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3PuEqHj9mFC"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHJIHRhz9mFC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch68Ue2B9mFC"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glAGPLde9mFC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSvHBTAa9mFC"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0KYhPKf9mFD"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqk3kuXLN55h"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlT5s2WRN55h"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIGKYV4MN55i"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0akqYr6N55i"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMgnJrjrN55i"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SziP1AejN55i"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOXgneXN-AL7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=5)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBbwZHyS9m1x"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGkSfkRD9m1y"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WovKgbpN9m1y"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxn_GvzY9m1y"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hahKjcXC9m1y"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vaFTI5q9m1y"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOt79IlH9m1y"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebT_G6SD9m1z"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee_ZHdfP9m1z"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cme_0zifN-8L"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCNInIXwN-8L"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYIKmqqPN-8L"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SbqdT_cN-8M"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUmeyvDPN-8M"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au45sozIN-8M"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_wdDZJT-Gd3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=6)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eg0EuGb9nWF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5n4QXbF9nWG"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51KZpTs_9nWG"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06C2NaYa9nWG"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA5f0kJK9nWG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhrKPgQm9nWH"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSDhSTFE9nWH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acPS6Rl59nWH"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqjld2-K9nWH"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuJeddc4OYnn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x9uSXsdOYnn"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X39-W18nOYno"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLYQ9NoxOYno"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfGfdUtsOYno"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVU4uAQFOYno"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8J53Nmi-JTE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=7)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmzthcMC9n14"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecOY76899n14"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF5Yq61U9n14"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_I4TVal9n14"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN0dj1ur9n14"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZyGKQF09n15"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THz3iYQ39n15"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pummlms9n15"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4etH6d109n15"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWATJhQsOi7t"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS0cI_VEOi7u"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8lGcgMDOi7u"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp1_qJt4Oi7u"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsQdcQiqOi7u"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMcfF2BvOi7u"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uCgGJ8t-QeZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=8)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YraEXbP19oRH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2N5z-UO9oRH"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ZmWZy09oRI"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RJHUqVK9oRI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_wDs54Z9oRI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d6jtumu9oRJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjbVhs9q9oRJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPB3t27q9oRJ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTSEB95k9oRJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwuhZp9LOnKv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU2wnWDmOnKv"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jKyDFPpOnKw"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TTK0QlZOnKw"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a7QP7kJOnKw"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFDiLz7TOnKw"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4lgWLe3-Tud"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=9)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S1OiE4W9omI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44FMIral9omI"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25U1W6zz9omJ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0az6M3cU9omJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs1K7XIO9omJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFbdf_Mv9omJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOzfpmLC9omJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR8AsE2z9omJ"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm_4ZV229omK"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDVcd0AG-a1W"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=10)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4HRc41h9rPZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_UmDL_S9rPZ"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URcfmbU39rPZ"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK8N5zx39rPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ_NF2Xx9rPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6avYzgXX9rPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI6KCkd3Ota7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGyZhRGOta8"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgRmD0BUOta8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxSVXGusOta9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkXkl0nbOta9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2HecuIoOta9"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee1Lpqgc9rPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqevIFOe9rPa"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcuP7h179rPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlIhrHHl-eE3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "selector = SelectKBest(score_func=chi2, k=11)  # Choose your desired feature selection method and set the number of features to select\n",
        "X_selected = selector.fit_transform(X, y)  # Apply feature selection to the original features\n",
        "\n",
        "# Print the indices of the selected features\n",
        "print(selector.get_support(indices=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI60gAfl9rxc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test= train_test_split(X_selected, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGn7nv2I9rxc"
      },
      "outputs": [],
      "source": [
        "X_train1, X_val, y_train1, y_val= train_test_split(X_train, y_train, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y_6sSBq9rxc"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO0B7wry9rxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjQNLMda9rxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import  RandomizedSearchCV\n",
        "\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 160, 170,180,200,220, 240, 250, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 3,4,5,6,7,8, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Create a Random Forest classifier with default hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation to find the best hyperparameters\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100,\n",
        "                               cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
        "rf_random.fit(X_train1, y_train1)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best hyperparameters:\", rf_random.best_params_)\n",
        "\n",
        "# Train the classifier on the training set using the best hyperparameters\n",
        "rf = RandomForestClassifier(random_state=42, **rf_random.best_params_)\n",
        "rf.fit(X_train1, y_train1)\n",
        "\n",
        "# Validate the classifier using 10-fold cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train1, y_train1, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "# Test the classifier on the training set\n",
        "y_pred1 = rf.predict(X_val)\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF3m-Uxu9rxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dTree_clf = DecisionTreeClassifier()\n",
        "dTree_clf.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(dTree_clf, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= dTree_clf.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = dTree_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\\n\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\\n\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\\n\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmcLRyuF9rxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toVFaMVh9rxd"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 0.5]\n",
        "}\n",
        "xgb_classifier = GridSearchCV(xgb_clf, param_grid=param_grid, cv=5)\n",
        "xgb_classifier.fit(X_train1,y_train1)\n",
        "cv_scores = cross_val_score(xgb_classifier, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= xgb_classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "auc_score1 = roc_auc_score(y_val, y_pred1)\n",
        "# Test the classifier on the testing set\n",
        "y_pred = xgb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "auc_score = roc_auc_score(y_test, y_pred)\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"AUC:\", auc_score1)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"AUC:\", auc_score)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V15Fpj19rxd"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"  \n",
        "classifier = SVC(kernel='linear', random_state=0)  \n",
        "classifier.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(classifier, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= classifier.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNrs3MJgOyYQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# create a decision tree classifier as the base estimator\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# create an AdaBoost classifier with 50 estimators and a learning rate of 1.0\n",
        "adaboost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=1)\n",
        "adaboost.fit(X_train1, y_train1)  \n",
        "cv_scores = cross_val_score(adaboost, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "adaboost.fit(X_train1, y_train1)\n",
        "y_pred1= adaboost.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgWqeEHyOyYR"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Create Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train1, y_train1) \n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(nb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nb.fit(X_train1, y_train1)\n",
        "y_pred1= nb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xob8XcuzOyYR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic_regression = LogisticRegression(random_state=1)\n",
        "logistic_regression.fit(X_train1, y_train1)  \n",
        "# perform 5-fold cross-validation on the training set\n",
        "cv_scores = cross_val_score(logistic_regression, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "logistic_regression.fit(X_train1, y_train1)\n",
        "y_pred1= logistic_regression.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = logistic_regression.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffKWYwk7OyYS"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Create Gradient Boosting model\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(gb, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "y_pred1= gb.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = gb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmm-9W5oOyYS"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier()\n",
        "cv_scores = cross_val_score(nn, X_train1, y_train1, cv=5)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "nn.fit(X_train1, y_train1)\n",
        "y_pred1= nn.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = nn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqbnANgUOyYS"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "lgbm.fit(X_train1, y_train1)\n",
        "# Evaluate model using 5-fold cross-validation\n",
        "cv_scores = cross_val_score(lgbm, X_train1, y_train1, cv=10)\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "print(\"Average cross-validation score:\", cv_scores.mean())\n",
        "y_pred1= lgbm.predict(X_val)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy1 = accuracy_score(y_val, y_pred1)\n",
        "precision1 = precision_score(y_val, y_pred1)\n",
        "recall1 = recall_score(y_val, y_pred1)\n",
        "f11 = f1_score(y_val, y_pred1)\n",
        "confusion1 = confusion_matrix(y_val, y_pred1)\n",
        "\n",
        "# Test the classifier on the testing set\n",
        "y_pred = lgbm.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Training===================\")\n",
        "print(\"Accuracy training:\", accuracy1)\n",
        "print(\"Precision training:\", precision1)\n",
        "print(\"Recall training:\", recall1)\n",
        "print(\"F1 score training:\", f11)\n",
        "print(\"Confusion matrix:\\n\", confusion1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"================== Testing===================\")\n",
        "print(\"Accuracy testing:\", accuracy)\n",
        "print(\"Precision testing:\", precision)\n",
        "print(\"Recall testing:\", recall)\n",
        "print(\"F1 score testing:\", f1)\n",
        "print(\"Confusion matrix:\\n\", confusion)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}